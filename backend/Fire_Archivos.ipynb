{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, re, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# =================== CONFIG ===================\n",
        "FILEPATH   = \"/content/fire_archive_M6_156000.csv\"   # <-- Cambia por tu Excel o CSV\n",
        "TARGET_COL = None                # <-- opcional: ej. \"PM2.5\", \"frp\", \"brightness\"\n",
        "MISSING_THRESHOLD = 0.40         # columnas con >40% NA se eliminan\n",
        "RANDOM_STATE = 42\n",
        "# ==============================================\n",
        "\n",
        "# ---------- Utilidades ----------\n",
        "def load_any_table(path: str) -> pd.DataFrame:\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext in [\".xls\", \".xlsx\"]:\n",
        "        return pd.read_excel(path)\n",
        "    # CSV: primero intento estándar, luego con ';'\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, sep=\";\")\n",
        "\n",
        "def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = (df.columns\n",
        "                  .str.strip()\n",
        "                  .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "                  .str.replace(r\"[^\\w]\", \"\", regex=True)\n",
        "                  .str.lower())\n",
        "    return df\n",
        "\n",
        "def parse_date_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == \"object\":\n",
        "            parsed = pd.to_datetime(df[col], errors=\"coerce\")\n",
        "            if parsed.notna().mean() >= 0.5:  # si la mitad o más parecen fechas\n",
        "                df[col] = parsed\n",
        "    return df\n",
        "\n",
        "def basic_cleaning(df: pd.DataFrame, missing_thresh: float = 0.4) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # 1) quitar columnas con demasiados NA\n",
        "    df = df.loc[:, df.isna().mean() <= missing_thresh]\n",
        "    # 2) quitar columnas constantes\n",
        "    df = df.loc[:, df.nunique(dropna=True) > 1]\n",
        "    # 3) intentar convertir objetos numéricos (coma decimal)\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == \"object\":\n",
        "            try:\n",
        "                s = df[c].str.replace(\",\", \".\", regex=False)\n",
        "                df[c] = pd.to_numeric(s, errors=\"ignore\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    # 4) ordenar por la primera columna datetime si existe\n",
        "    date_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n",
        "    if date_cols:\n",
        "        df = df.sort_values(date_cols[0]).reset_index(drop=True)\n",
        "    # 5) duplicados\n",
        "    return df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "def quick_eda(df: pd.DataFrame, target: str | None):\n",
        "    print(\"\\n=== FORMA DEL DATASET ===\")\n",
        "    print(df.shape)\n",
        "    print(\"\\n=== TIPOS ===\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\n=== NULOS (proporción) ===\")\n",
        "    print(df.isna().mean().sort_values(ascending=False).head(20))\n",
        "    if target and target in df:\n",
        "        print(f\"\\n=== DESCRIPTIVOS de '{target}' ===\")\n",
        "        print(df[target].describe())\n",
        "\n",
        "def select_target_and_features(df: pd.DataFrame, target=None):\n",
        "    numeric_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
        "    if not numeric_cols:\n",
        "        raise ValueError(\"No hay columnas numéricas para modelar.\")\n",
        "    if target is None:\n",
        "        priority = [\"frp\",\"pm25\",\"pm2_5\",\"pm10\",\"o3\",\"no2\",\"so2\",\"co\",\"brightness\",\"value\",\"target\",\"y\"]\n",
        "        lower = {c: c.lower() for c in df.columns}\n",
        "        for p in priority:\n",
        "            for c in numeric_cols:\n",
        "                if lower[c] == p:\n",
        "                    target = c; break\n",
        "            if target: break\n",
        "        if target is None:\n",
        "            # elegir la numérica con menos NA y mayor varianza\n",
        "            candidates = []\n",
        "            for c in numeric_cols:\n",
        "                candidates.append((c, df[c].isna().mean(), df[c].var(skipna=True)))\n",
        "            candidates = sorted(candidates, key=lambda x: (x[1], -0 if pd.isna(x[2]) else -x[2]))\n",
        "            target = candidates[0][0]\n",
        "    features = [c for c in numeric_cols if c != target]\n",
        "    # si hay demasiadas, quedarse con las 30 más correlacionadas\n",
        "    if len(features) > 30:\n",
        "        corr = df[features + [target]].corr(numeric_only=True)[target].abs().sort_values(ascending=False)\n",
        "        features = corr.index.tolist()[1:31]\n",
        "    return target, features\n",
        "\n",
        "def build_models(feats):\n",
        "    # Preprocesamiento (escalado) solo para LR\n",
        "    preproc = ColumnTransformer([(\"num\", StandardScaler(), feats)], remainder=\"drop\")\n",
        "    lr = Pipeline([(\"prep\", preproc), (\"model\", LinearRegression())])\n",
        "    rf = Pipeline([(\"model\", RandomForestRegressor(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1))])\n",
        "    return {\"LinearRegression\": lr, \"RandomForest\": rf}\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    return {\n",
        "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"MSE\": float(mse),\n",
        "        \"RMSE\": float(mse ** 0.5),\n",
        "        \"R2\": float(r2_score(y_true, y_pred))\n",
        "    }\n",
        "\n",
        "# --------- Gráficas (matplotlib puro) ---------\n",
        "def plot_hist(series: pd.Series, title: str):\n",
        "    plt.figure()\n",
        "    series.dropna().plot(kind=\"hist\", bins=30)\n",
        "    plt.title(title); plt.xlabel(series.name if series.name else \"\"); plt.ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_timeseries(df: pd.DataFrame, target: str):\n",
        "    date_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n",
        "    if not date_cols: return\n",
        "    col = date_cols[0]\n",
        "    plt.figure()\n",
        "    plt.plot(df[col], df[target])\n",
        "    plt.title(f\"Serie temporal de {target}\"); plt.xlabel(col); plt.ylabel(target)\n",
        "    plt.show()\n",
        "\n",
        "def plot_real_vs_pred(y_true, y_pred, title: str):\n",
        "    lim_min = min(np.min(y_true), np.min(y_pred))\n",
        "    lim_max = max(np.max(y_true), np.max(y_pred))\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=10)\n",
        "    plt.plot([lim_min, lim_max], [lim_min, lim_max])\n",
        "    plt.title(title); plt.xlabel(\"Real\"); plt.ylabel(\"Predicción\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_residuals(y_true, y_pred):\n",
        "    resid = y_true - y_pred\n",
        "    plt.figure()\n",
        "    plt.hist(resid, bins=30)\n",
        "    plt.title(\"Histograma de residuos\"); plt.xlabel(\"Residuo\"); plt.ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance(model, feats):\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        imp = model.feature_importances_\n",
        "        order = np.argsort(imp)[::-1]\n",
        "        plt.figure(figsize=(8, max(3, len(feats) * 0.25)))\n",
        "        plt.bar(range(len(feats)), imp[order])\n",
        "        plt.xticks(range(len(feats)), [feats[i] for i in order], rotation=90)\n",
        "        plt.title(\"Importancia de variables (RandomForest)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# =================== EJECUCIÓN ===================\n",
        "def main():\n",
        "    # 1) Cargar\n",
        "    df = load_any_table(FILEPATH)\n",
        "    df = standardize_column_names(df)\n",
        "    df = parse_date_columns(df)\n",
        "    # 2) Limpiar/filtrar/ordenar\n",
        "    df = basic_cleaning(df, MISSING_THRESHOLD)\n",
        "    # 3) EDA rápida\n",
        "    target, features = select_target_and_features(df, TARGET_COL)\n",
        "    quick_eda(df, target)\n",
        "\n",
        "    # 4) Preparar datos (quita NA en features y target)\n",
        "    model_df = df[features + [target]].dropna().copy()\n",
        "    X = model_df[features]        # DataFrame (para que ColumnTransformer pueda usar nombres)\n",
        "    y = model_df[target].values   # numpy\n",
        "\n",
        "    # 5) Split\n",
        "    test_size = 0.2 if len(model_df) >= 50 else (0.2 if len(model_df) >= 10 else 0.4)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
        "                                                        random_state=RANDOM_STATE)\n",
        "\n",
        "    # 6) Modelar\n",
        "    models = build_models(features)\n",
        "    results, trained = {}, {}\n",
        "    for name, mdl in models.items():\n",
        "        mdl.fit(X_train, y_train)\n",
        "        yhat = mdl.predict(X_test)\n",
        "        results[name] = evaluate(y_test, yhat)\n",
        "        trained[name] = mdl\n",
        "\n",
        "    # 7) Elegir mejor por R2\n",
        "    best_name = max(results, key=lambda k: results[k][\"R2\"])\n",
        "    best_model = trained[best_name]\n",
        "\n",
        "    # 8) Predicciones y guardado\n",
        "    preds_all = best_model.predict(model_df[features])\n",
        "    out = model_df.copy()\n",
        "    out[\"y_pred\"] = preds_all\n",
        "    out[\"residual\"] = out[target] - out[\"y_pred\"]\n",
        "\n",
        "    clean_path = \"dataset_limpio.csv\"\n",
        "    preds_path = \"predicciones.csv\"\n",
        "    df.to_csv(clean_path, index=False)\n",
        "    out.to_csv(preds_path, index=False)\n",
        "\n",
        "    print(\"\\n=== RESUMEN ===\")\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"Features ({len(features)}): {features[:10]}{'...' if len(features)>10 else ''}\")\n",
        "    print(\"Métricas:\")\n",
        "    for m, sc in results.items():\n",
        "        print(f\"  {m}: {sc}\")\n",
        "\n",
        "    print(f\"\\nArchivos guardados:\\n - {os.path.abspath(clean_path)}\\n - {os.path.abspath(preds_path)}\")\n",
        "\n",
        "    # 9) Gráficas\n",
        "    plot_hist(model_df[target], f\"Histograma de {target}\")\n",
        "    plot_timeseries(df, target)           # si hay fechas\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "    plot_real_vs_pred(y_test, y_test_pred, f\"Real vs Predicción ({best_name})\")\n",
        "    plot_residuals(y_test, y_test_pred)\n",
        "    if best_name == \"RandomForest\":\n",
        "        plot_feature_importance(best_model.named_steps[\"model\"], features)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyrbctvsaTR4",
        "outputId": "9d7c5299-a4ab-48a4-fb90-a0bc2e5aa628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2807978596.py:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(df[col], errors=\"coerce\")\n",
            "/tmp/ipython-input-2807978596.py:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(df[col], errors=\"coerce\")\n",
            "/tmp/ipython-input-2807978596.py:62: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df[c] = pd.to_numeric(s, errors=\"ignore\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FORMA DEL DATASET ===\n",
            "(1182272, 13)\n",
            "\n",
            "=== TIPOS ===\n",
            "latitude             float64\n",
            "longitude            float64\n",
            "brightness           float64\n",
            "scan                 float64\n",
            "track                float64\n",
            "acq_date      datetime64[ns]\n",
            "acq_time               int64\n",
            "satellite             object\n",
            "confidence             int64\n",
            "bright_t31           float64\n",
            "frp                  float64\n",
            "daynight              object\n",
            "type                   int64\n",
            "dtype: object\n",
            "\n",
            "=== NULOS (proporción) ===\n",
            "latitude      0.0\n",
            "longitude     0.0\n",
            "brightness    0.0\n",
            "scan          0.0\n",
            "track         0.0\n",
            "acq_date      0.0\n",
            "acq_time      0.0\n",
            "satellite     0.0\n",
            "confidence    0.0\n",
            "bright_t31    0.0\n",
            "frp           0.0\n",
            "daynight      0.0\n",
            "type          0.0\n",
            "dtype: float64\n",
            "\n",
            "=== DESCRIPTIVOS de 'frp' ===\n",
            "count    1.182272e+06\n",
            "mean     6.701266e+01\n",
            "std      2.152388e+02\n",
            "min     -5.920000e+01\n",
            "25%      1.090000e+01\n",
            "50%      2.250000e+01\n",
            "75%      5.310000e+01\n",
            "max      1.437650e+04\n",
            "Name: frp, dtype: float64\n"
          ]
        }
      ]
    }
  ]
}